yco <- 0.07
epa <- 0.07
elu <- 0.06
avoid <- 0.06
brk <- 0.06
td <- 0.065
success <- 0.05
stacked <- 0.05
yrr <- 0.05
fumbles <- 0.02
# Assign the SNC grade
rb_stats$snc_grade <- rb_stats$yards.touch * yards_touch +
rb_stats$ryoe.att * ryoe +
rb_stats$yco_attempt * yco +
rb_stats$avoided.att * avoid +
rb_stats$breakaway_percent * brk +
rb_stats$grades_offense * pff +
rb_stats$touches.game * touches +
rb_stats$touchdowns.touch * td +
rb_stats$stacked_pct * stacked +
rb_stats$yprr * yrr +
rb_stats$success_rate * success +
rb_stats$epa.att * epa +
rb_stats$elusive_rating * elu +
rb_stats$fumbles.touch * fumbles
# Scale the data to reflect realistic grades
min_snc <- min(rb_stats$snc_grade, na.rm = TRUE)
max_snc <- max(rb_stats$snc_grade, na.rm = TRUE)
rb_stats$rb_grade <- 50 + (49 * (rb_stats$snc_grade - min_snc) / (max_snc - min_snc))
rb_sorted <- rb_stats[order(-rb_stats$rb_grade), c("player", "rb_grade")]
rb_sorted$ranking <- seq_len(nrow(rb_sorted))  # Create ranking column
rb_sorted <- rb_sorted[, c("ranking", "player", "rb_grade")]  # Reorder columns
print(rb_sorted[1:20, ], row.names = FALSE)  # Print next 20
print(rb_sorted[21:41, ], row.names = FALSE)  # Print next 20
rb_duos <- rb_stats %>%
group_by(team_name) %>%
slice_max(order_by = snap_counts_defense, n = 2, with_ties = FALSE) %>%
mutate(team_grade = mean(rb_grade, na.rm = TRUE)) %>%
ungroup()
rb_duos <- rb_stats %>%
group_by(team_name) %>%
slice_max(order_by = attempts, n = 2, with_ties = FALSE) %>%
mutate(team_grade = mean(rb_grade, na.rm = TRUE)) %>%
ungroup()
View(rb_duos)
rb_duos <- rb_stats %>%
group_by(team_name) %>%
slice_max(order_by = attempts, n = 2, with_ties = FALSE) %>%
mutate(team_grade = weighted.mean(rb_grade, attempts, na.rm = TRUE)) %>%
ungroup()
#SNC RB Grades 2024-2025
library(dplyr)
# Load data
pff_rb <- read.csv("rushing_summary (6).csv")
pff_rb <- pff_rb %>% filter(position == "HB")
rbs <- read.csv("2024_runningbacks - Sheet1.csv")
rbs$year <- 2024
rbs2 <- read.csv("rb_duos_data.csv")
# Join
rbs <- bind_rows(rbs, rbs2)
rbs <- rbs[-c(79:119), ]
rbs <- rbs[-c(28, 29, 40, 31, 48, 49, 53, 54, 57, 60, 62, 63, 66), ]
# Select the metrics you want to use
rb_stats <- rbs %>% select(player, player_game_count, ryoe.att, epa.att, success_rate, stacked_pct, attempts, avoided_tackles, breakaway_percent,
elusive_rating, explosive, fumbles, grades_offense, total_touches, touchdowns, yards, yco_attempt,
ypa, yprr, rec_yards, team_name)
# Stat engineering
rb_stats <- rb_stats %>%
mutate(avoided.att = avoided_tackles / attempts) %>%
mutate(explosive.att = explosive /attempts) %>%
mutate(fumbles.touch = fumbles / total_touches) %>%
mutate(touchdowns.touch = touchdowns / total_touches) %>%
mutate(yards.touch = (rec_yards + yards) / total_touches) %>%
mutate(touches.game = total_touches / player_game_count)
# Negate fumbles
rb_stats <- rb_stats %>%
mutate(fumbles.touch = -fumbles.touch)
# Standardize the values
rb_stats$player <- as.character(rb_stats$player)
rb_stats$team_name <- as.character(rb_stats$team_name)
rb_stats_scaled <- as.data.frame(scale(rb_stats[, sapply(rb_stats, is.numeric)]))
rb_stats_scaled$player <- rb_stats$player
rb_stats_scaled$team_name <- rb_stats$team_name
rb_stats <- rb_stats_scaled
# Remove the QBs
rb_stats <- rb_stats %>% filter(player != "Lamar Jackson" & player != "Jalen Hurts" & player != "Josh Allen" & player != "Jayden Daniels")
# Assign weights to metrics
yards_touch <- 0.125
touches <- 0.12
ryoe <- 0.1
pff <- 0.1
yco <- 0.07
epa <- 0.07
elu <- 0.06
avoid <- 0.06
brk <- 0.06
td <- 0.065
success <- 0.05
stacked <- 0.05
yrr <- 0.05
fumbles <- 0.02
# Assign the SNC grade
rb_stats$snc_grade <- rb_stats$yards.touch * yards_touch +
rb_stats$ryoe.att * ryoe +
rb_stats$yco_attempt * yco +
rb_stats$avoided.att * avoid +
rb_stats$breakaway_percent * brk +
rb_stats$grades_offense * pff +
rb_stats$touches.game * touches +
rb_stats$touchdowns.touch * td +
rb_stats$stacked_pct * stacked +
rb_stats$yprr * yrr +
rb_stats$success_rate * success +
rb_stats$epa.att * epa +
rb_stats$elusive_rating * elu +
rb_stats$fumbles.touch * fumbles
# Scale the data to reflect realistic grades
min_snc <- min(rb_stats$snc_grade, na.rm = TRUE)
max_snc <- max(rb_stats$snc_grade, na.rm = TRUE)
rb_stats$rb_grade <- 50 + (49 * (rb_stats$snc_grade - min_snc) / (max_snc - min_snc))
rb_sorted <- rb_stats[order(-rb_stats$rb_grade), c("player", "rb_grade")]
rb_sorted$ranking <- seq_len(nrow(rb_sorted))  # Create ranking column
rb_sorted <- rb_sorted[, c("ranking", "player", "rb_grade")]  # Reorder columns
print(rb_sorted[1:20, ], row.names = FALSE)  # Print next 20
print(rb_sorted[21:41, ], row.names = FALSE)  # Print next 20
rb_duos <- rb_stats %>%
group_by(team_name) %>%
slice_max(order_by = attempts, n = 2, with_ties = FALSE) %>%
mutate(team_grade = weighted.mean(rb_grade, attempts, na.rm = TRUE)) %>%
ungroup()
rb_duos <- rb_stats %>%
group_by(team_name) %>%
slice_max(order_by = attempts, n = 2, with_ties = FALSE) %>%
mutate(team_grade = 0.65 * rb_grade[1] + 0.35 * rb_grade[2]) %>%
ungroup()
rb_duos <- rb_stats %>%
group_by(team_name) %>%
slice_max(order_by = attempts, n = 2, with_ties = FALSE) %>%
mutate(team_grade = 0.70 * rb_grade[1] + 0.30 * rb_grade[2]) %>%
ungroup()
getwd()
setwd("/Users/gavinbulthuis/Desktop/stat4051/Final Project/Activity-Clustering-using-Motion-Sensors")
library(tidyverse)
library(dplyr)
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(corrr)
library(ggrepel)
library(kernlab)
library(knitr)
library(clValid)
# Create column names based on sensor location
unit_names <- c("Torso", "RightArm", "LeftArm", "RightLeg", "LeftLeg")
column_names <- unlist(lapply(unit_names, function(unit) {
paste0(unit, "_S", 1:9)
}))
# Vector of activity folder names
activities <- sprintf("a%02d", 01:19)
# Function to read sensor data for an activity
read_activity_data <- function(activities) {
files <- list.files(
path = file.path("data", activities, "p1"),
recursive = TRUE,
pattern = "\\.txt$",
full.names = TRUE
)
map_dfr(files, ~ read_csv(.x, col_names = FALSE)) %>%
set_names(column_names)
}
sensor_data_list <- imap(activities, ~ read_activity_data(.x))
names(sensor_data_list) <- activities
# Plot first 3 torso sensors for first activity (sitting)
sitting_data <- sensor_data_list$a01[,1:3] %>%
mutate(Time = 1:n()) %>%
pivot_longer(-Time, names_to = "Sensor")
ggplot(sitting_data, aes(Time, value, color = Sensor)) +
geom_line(alpha = 0.7) +
facet_wrap(~Sensor, ncol = 1, scales = "free_y") +
labs(title = "Raw Sensor Signals: Sitting Activity (Torso)",
subtitle = "First 3 sensors (x/y/z accelerometers)",
y = "Sensor Value")
# Create column names based on sensor location
unit_names <- c("Torso", "RightArm", "LeftArm", "RightLeg", "LeftLeg")
column_names <- unlist(lapply(unit_names, function(unit) {
paste0(unit, "_S", 1:9)
}))
# Vector of activity folder names
activities <- sprintf("a%02d", 01:19)
# Function to read sensor data for an activity
read_activity_data <- function(activities) {
files <- list.files(
path = file.path("/Users/gavinbulthuis/Desktop/stat4051/Final Project/Activity-Clustering-using-Motion-Sensors/data", activities, "p1"),
recursive = TRUE,
pattern = "\\.txt$",
full.names = TRUE
)
map_dfr(files, ~ read_csv(.x, col_names = FALSE)) %>%
set_names(column_names)
}
sensor_data_list <- imap(activities, ~ read_activity_data(.x))
names(sensor_data_list) <- activities
View(rb_duos)
unit_names <- c("Torso", "RightArm", "LeftArm", "RightLeg", "LeftLeg")
# Function to extract features for each file
feature_extraction <- function(df) {
unit_features <- map_dfr(unit_names, function(unit) {
unit_data <- df %>% select(starts_with(unit))
tibble(
unit = unit,
mean = mean(unlist(unit_data), na.rm = TRUE),
sd = sd(unlist(unit_data), na.rm = TRUE),
max = max(unlist(unit_data), na.rm = TRUE),
min = min(unlist(unit_data), na.rm = TRUE),
bottom_quarter = quantile(unlist(unit_data), probs = 0.25, na.rm = TRUE),
top_quarter = quantile(unlist(unit_data), probs = 0.75, na.rm = TRUE),
median = median(unlist(unit_data), na.rm = TRUE)
)
})
# unit_features %>%
#   pivot_wider(names_from = unit, values_from = c(mean, sd, median, max, min, bottom_quarter, top_quarter), names_glue = "{unit}_{.value}")
}
# Loop through and apply to all of the activities
activity_features <- map_dfr(sensor_data_list, feature_extraction, .id = "activity")
# Attach the features back to the data
attach_features <- function(df) {
features <- feature_extraction(df)
features_widened <- pivot_wider(features, names_from = unit,
values_from = c(mean, sd, max, min, bottom_quarter, top_quarter, median), names_sep = "_")
features_repeated <- features_widened[rep(1, nrow(df)), ]
bind_cols(df, features_repeated)
}
sensor_data_w_features <- map(sensor_data_list, attach_features)
# Initialize matrix to store activities
activity_matrix <- list()
for (i in names(sensor_data_w_features)) {
# Raw data
data <- sensor_data_list[[i]][, 1:45] # Use only raw data
sensor_data_scaled <- scale(data) # Scale raw data
pca_sensor_data <- prcomp(sensor_data_scaled, center = TRUE, scale. = TRUE) # Apply PCA
pcs <- 20 # Choose 20 PCs to keep approx, 80% of cumulative variance
reduced_vector <- as.vector(t(pca_sensor_data$x[, 1:pcs])) # Turn into a row-wise vector
# Extracted feature data
features <- as.data.frame(sensor_data_w_features[[i]][, 46:ncol(sensor_data_w_features[[i]])])
features <- as.vector(unlist(features))
# Combine
full_vec <- c(reduced_vector, features)
activity_matrix[[i]] <- full_vec
}
# Hierarchical Clustering
hc <- hclust(dist(activity_matrix), method = "ward.D2")
activity_matrix <- do.call(rbind, activity_matrix)
# Hierarchical Clustering
hc <- hclust(dist(activity_matrix), method = "ward.D2")
hc$labels <- c("Sitting", "Standing", "Lying on Back", "Lying on Side", "Ascending
Stairs","Descending Stairs", "Standing in Elevator", "Moving in Elevator",
"Walking Parking Lot", "Walking on Treadmill",
"Walking on Inclined Treadmill", "Running on Treadmill",
"Exercising on Stairmaster", "Exercising on Elliptical",
"Stationary Bike in Horizontal Position",
"Stationary Bike in Vertical  Position", "Rowing", "Jumping",
"Playing  Basketball")
plot(hc, cex = 0.5)
View(hc)
# Create column names based on sensor location
unit_names <- c("Torso", "RightArm", "LeftArm", "RightLeg", "LeftLeg")
column_names <- unlist(lapply(unit_names, function(unit) {
paste0(unit, "_S", 1:9)
}))
# Vector of activity folder names
activities <- sprintf("a%02d", 01:19)
# Function to read sensor data for an activity
read_activity_data <- function(activities) {
files <- list.files(
path = file.path("/Users/gavinbulthuis/Desktop/stat4051/Final Project/Activity-Clustering-using-Motion-Sensors/data", activities, "p2"),
recursive = TRUE,
pattern = "\\.txt$",
full.names = TRUE
)
map_dfr(files, ~ read_csv(.x, col_names = FALSE)) %>%
set_names(column_names)
}
sensor_data_list <- imap(activities, ~ read_activity_data(.x))
names(sensor_data_list) <- activities
unit_names <- c("Torso", "RightArm", "LeftArm", "RightLeg", "LeftLeg")
# Function to extract features for each file
feature_extraction <- function(df) {
unit_features <- map_dfr(unit_names, function(unit) {
unit_data <- df %>% select(starts_with(unit))
tibble(
unit = unit,
mean = mean(unlist(unit_data), na.rm = TRUE),
sd = sd(unlist(unit_data), na.rm = TRUE),
max = max(unlist(unit_data), na.rm = TRUE),
min = min(unlist(unit_data), na.rm = TRUE),
bottom_quarter = quantile(unlist(unit_data), probs = 0.25, na.rm = TRUE),
top_quarter = quantile(unlist(unit_data), probs = 0.75, na.rm = TRUE),
median = median(unlist(unit_data), na.rm = TRUE)
)
})
# unit_features %>%
#   pivot_wider(names_from = unit, values_from = c(mean, sd, median, max, min, bottom_quarter, top_quarter), names_glue = "{unit}_{.value}")
}
# Loop through and apply to all of the activities
activity_features <- map_dfr(sensor_data_list, feature_extraction, .id = "activity")
# Attach the features back to the data
attach_features <- function(df) {
features <- feature_extraction(df)
features_widened <- pivot_wider(features, names_from = unit,
values_from = c(mean, sd, max, min, bottom_quarter, top_quarter, median), names_sep = "_")
features_repeated <- features_widened[rep(1, nrow(df)), ]
bind_cols(df, features_repeated)
}
sensor_data_w_features <- map(sensor_data_list, attach_features)
# Initialize matrix to store activities
activity_matrix <- list()
for (i in names(sensor_data_w_features)) {
# Raw data
data <- sensor_data_list[[i]][, 1:45] # Use only raw data
sensor_data_scaled <- scale(data) # Scale raw data
pca_sensor_data <- prcomp(sensor_data_scaled, center = TRUE, scale. = TRUE) # Apply PCA
pcs <- 20 # Choose 20 PCs to keep approx, 80% of cumulative variance
reduced_vector <- as.vector(t(pca_sensor_data$x[, 1:pcs])) # Turn into a row-wise vector
# Extracted feature data
features <- as.data.frame(sensor_data_w_features[[i]][, 46:ncol(sensor_data_w_features[[i]])])
features <- as.vector(unlist(features))
# Combine
full_vec <- c(reduced_vector, features)
activity_matrix[[i]] <- full_vec
}
activity_matrix <- do.call(rbind, activity_matrix)
# #  KMeans
# # Select number of activities
# k <- 7
#
# # Run KMeans
# kmeans_result <- kmeans(activity_matrix, centers = k, nstart = 100)
#
# fviz_nbclust(activity_matrix, kmeans, method = "wss")
# Spectral Clustering
# sc <- specc(activity_matrix, centers = 5)
#
# activity_data <- as.data.frame(activity_matrix)
# activity_data$Activity <- c("Sitting", "Standing", "Lying on Back", "Lying on Side", "Ascending Stairs",
#                              "Descending Stairs", "Standing in Elevator", "Moving in Elevator",
#                              "Walking Parking Lot", "Walking on Treadmill", "Walking on Inclined Treadmill",
#                              "Running on Treadmill", "Exercising on Stairmaster", "Exercising on Elliptical",
#                              "Stationary Bike in Horizontal Position", "Stationary Bike in Vertical Position",
#                              "Rowing", "Jumping", "Playing Basketball")
#
# # Cluster assignments before reducing dimensionality again
# clusters <- sc@.Data
# activity_data$Clusters <- clusters
#
# # New data frame to view clusters
# spectral <- activity_data %>% select(Activity, Clusters)
# Hierarchical Clustering
hc <- hclust(dist(activity_matrix), method = "ward.D2")
hc$labels <- c("Sitting", "Standing", "Lying on Back", "Lying on Side", "Ascending
Stairs","Descending Stairs", "Standing in Elevator", "Moving in Elevator",
"Walking Parking Lot", "Walking on Treadmill",
"Walking on Inclined Treadmill", "Running on Treadmill",
"Exercising on Stairmaster", "Exercising on Elliptical",
"Stationary Bike in Horizontal Position",
"Stationary Bike in Vertical  Position", "Rowing", "Jumping",
"Playing  Basketball")
plot(hc, cex = 0.5)
cutree(hc, k = 5)
plot(hc, cex = 0.5)
# Create column names based on sensor location
unit_names <- c("Torso", "RightArm", "LeftArm", "RightLeg", "LeftLeg")
column_names <- unlist(lapply(unit_names, function(unit) {
paste0(unit, "_S", 1:9)
}))
# Vector of activity folder names
activities <- sprintf("a%02d", 01:19)
# Function to read sensor data for an activity
read_activity_data <- function(activities) {
files <- list.files(
path = file.path("/Users/gavinbulthuis/Desktop/stat4051/Final Project/Activity-Clustering-using-Motion-Sensors/data", activities, "p1"),
recursive = TRUE,
pattern = "\\.txt$",
full.names = TRUE
)
map_dfr(files, ~ read_csv(.x, col_names = FALSE)) %>%
set_names(column_names)
}
sensor_data_list <- imap(activities, ~ read_activity_data(.x))
names(sensor_data_list) <- activities
unit_names <- c("Torso", "RightArm", "LeftArm", "RightLeg", "LeftLeg")
# Function to extract features for each file
feature_extraction <- function(df) {
unit_features <- map_dfr(unit_names, function(unit) {
unit_data <- df %>% select(starts_with(unit))
tibble(
unit = unit,
mean = mean(unlist(unit_data), na.rm = TRUE),
sd = sd(unlist(unit_data), na.rm = TRUE),
max = max(unlist(unit_data), na.rm = TRUE),
min = min(unlist(unit_data), na.rm = TRUE),
bottom_quarter = quantile(unlist(unit_data), probs = 0.25, na.rm = TRUE),
top_quarter = quantile(unlist(unit_data), probs = 0.75, na.rm = TRUE),
median = median(unlist(unit_data), na.rm = TRUE)
)
})
# unit_features %>%
#   pivot_wider(names_from = unit, values_from = c(mean, sd, median, max, min, bottom_quarter, top_quarter), names_glue = "{unit}_{.value}")
}
# Loop through and apply to all of the activities
activity_features <- map_dfr(sensor_data_list, feature_extraction, .id = "activity")
# Attach the features back to the data
attach_features <- function(df) {
features <- feature_extraction(df)
features_widened <- pivot_wider(features, names_from = unit,
values_from = c(mean, sd, max, min, bottom_quarter, top_quarter, median), names_sep = "_")
features_repeated <- features_widened[rep(1, nrow(df)), ]
bind_cols(df, features_repeated)
}
sensor_data_w_features <- map(sensor_data_list, attach_features)
# Initialize matrix to store activities
activity_matrix <- list()
for (i in names(sensor_data_w_features)) {
# Raw data
data <- sensor_data_list[[i]][, 1:45] # Use only raw data
sensor_data_scaled <- scale(data) # Scale raw data
pca_sensor_data <- prcomp(sensor_data_scaled, center = TRUE, scale. = TRUE) # Apply PCA
pcs <- 20 # Choose 20 PCs to keep approx, 80% of cumulative variance
reduced_vector <- as.vector(t(pca_sensor_data$x[, 1:pcs])) # Turn into a row-wise vector
# Extracted feature data
features <- as.data.frame(sensor_data_w_features[[i]][, 46:ncol(sensor_data_w_features[[i]])])
features <- as.vector(unlist(features))
# Combine
full_vec <- c(reduced_vector, features)
activity_matrix[[i]] <- full_vec
}
activity_matrix <- do.call(rbind, activity_matrix)
# #  KMeans
# # Select number of activities
# k <- 7
#
# # Run KMeans
# kmeans_result <- kmeans(activity_matrix, centers = k, nstart = 100)
#
# fviz_nbclust(activity_matrix, kmeans, method = "wss")
# Spectral Clustering
# sc <- specc(activity_matrix, centers = 5)
#
# activity_data <- as.data.frame(activity_matrix)
# activity_data$Activity <- c("Sitting", "Standing", "Lying on Back", "Lying on Side", "Ascending Stairs",
#                              "Descending Stairs", "Standing in Elevator", "Moving in Elevator",
#                              "Walking Parking Lot", "Walking on Treadmill", "Walking on Inclined Treadmill",
#                              "Running on Treadmill", "Exercising on Stairmaster", "Exercising on Elliptical",
#                              "Stationary Bike in Horizontal Position", "Stationary Bike in Vertical Position",
#                              "Rowing", "Jumping", "Playing Basketball")
#
# # Cluster assignments before reducing dimensionality again
# clusters <- sc@.Data
# activity_data$Clusters <- clusters
#
# # New data frame to view clusters
# spectral <- activity_data %>% select(Activity, Clusters)
# Hierarchical Clustering
hc <- hclust(dist(activity_matrix), method = "ward.D2")
hc$labels <- c("Sitting", "Standing", "Lying on Back", "Lying on Side", "Ascending
Stairs","Descending Stairs", "Standing in Elevator", "Moving in Elevator",
"Walking Parking Lot", "Walking on Treadmill",
"Walking on Inclined Treadmill", "Running on Treadmill",
"Exercising on Stairmaster", "Exercising on Elliptical",
"Stationary Bike in Horizontal Position",
"Stationary Bike in Vertical  Position", "Rowing", "Jumping",
"Playing  Basketball")
plot(hc, cex = 0.5)
cutree(hc, 9)
plot(rev(hc$height), type = "b",
xlab = "Merge step", ylab = "Height",
main = "Elbow method for choosing number of clusters")
cutree(hc, 4)
library(cluster)
# Try several k values and compute average silhouette
sil_widths <- sapply(2:10, function(k) {
clusters <- cutree(hc, k = k)
silhouette_score <- silhouette(clusters, dist(activity_matrix))
mean(silhouette_score[, 3])
})
plot(2:10, sil_widths, type = "b",
xlab = "Number of clusters", ylab = "Average silhouette width",
main = "Silhouette method")
cutree(hc, 2)
cutree(hc, 10)
sort(cutree(hc, 10))
sort(cutree(hc, 9))
hc$labels <- c("Sitting", "Standing", "Lying on Back", "Lying on Side", "Ascending Stairs",
"Descending Stairs", "Standing in Elevator", "Moving in Elevator",
"Walking Parking Lot", "Walking on Treadmill",
"Walking on Inclined Treadmill", "Running on Treadmill",
"Exercising on Stairmaster", "Exercising on Elliptical",
"Stationary Bike in Horizontal Position",
"Stationary Bike in Vertical  Position", "Rowing", "Jumping",
"Playing  Basketball")
plot(hc, cex = 0.5)
sort(cutree(hc, 9))
-(4/13(1/2*log2(1/2) + 1/2*log2(1/2)) + 5/13(1/5*log2(1/5) + 4/5*log2(4/5)) + 4/13(1/2*log2(1/2) + 1/2*log2(1/2)))
(4/13(1/2*log2(1/2) + 1/2*log2(1/2)) + 5/13(1/5*log2(1/5) + 4/5*log2(4/5)) + 4/13(1/2*log2(1/2) + 1/2*log2(1/2)))
-(4/13 * (1/2*log2(1/2) + 1/2*log2(1/2)) + 5/13 * (1/5*log2(1/5) + 4/5*log2(4/5)) + 4/13 * (1/2*log2(1/2) + 1/2*log2(1/2)))
-(5/13 * (2/5*log2(2/5) + 3/5*log2(3/5)) + 4/13 * (1/2*log2(1/2) + 1/2*log2(1/2)) + 4/13 * (1/4*log2(1/4) + 3/4*log2(3/4)))
-(3/13 * (1*log2(1) + 0*log2(0)) + 4/13 * (1/2*log2(1/2) + 1/2*log2(1/2)) + 6/13 * (1*log2(1) + 0*log2(0)))
-(4/13 * (1/2*log2(1/2) + 1/2*log2(1/2)))
-(1/2 * (1/2*log2(1/2) + 1/2*log2(1/2)) + 1/2 * (1/2*log2(1/2) + 1/2*log2(1/2)))
-(1/2 * (1/2*log2(1/2) + 1/2*log2(1/2))
-(1/2 * (1/2*log2(1/2) + 1/2*log2(1/2)))
jk
-(1/2 * (1/2*log2(1/2) + 1/2*log2(1/2)))
